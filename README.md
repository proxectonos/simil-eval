# Similarity Framework
A tool for evaluating LLMs using similarity measures between embeddings.

> [!NOTE]  
> Currently, the tool is designed for use on the Cesga server. A standalone version for other servers will be available soon.

## Setup

1. Install the evaluation environment and create the necessary auxiliary folders:
    ```sh install.sh```
2. Edit the environment variables in the file ```./configs/.env```.

## How to Perform Cosine/MoverScore/BertScore Evaluations (QA Datasets)

1. Load the evaluation environment.
2. Navigate to the ```./launchers``` folder.
3. Update the following fields in the launcher file ```launch_similarity.es```:
    - **MODELS**: Models to evaluate. These can be references from HuggingFace or local paths.
    - **DATASETS**: Datasets to evaluate. Currently supports "openbookqa" and "belebele."
    - **LANGUAGES**: Languages of the dataset to evaluate. Currently available: gl, cat, es, en, pt.
    - **FEWSHOT_NUM**: Number of few-shot examples. To run evaluations without few-shot, set this to 0.
4. Run the script using ```sh launch_similarity.sh```, which will initiate processes in the Cesga queues (one for each dataset/model/language combination).

## How to Perform Minicons/Surprisal Evaluations

1. Load the evaluation environment.
2. Navigate to the ```./launchers``` folder.
3. Update the **MODELS** variable in the launcher corresponding to the desired dataset (Calame or CoLA).
4. Launch the process using ```sbatch``` on the Cesga queues:
   - **CoLA**: ```sbatch launch_CoLA```: Generates results for the CoLA versions in Galician, Catalan, Spanish, and English.
   - **Calame**: ```sbatch launch_Calame.sh```: Generates results for Calame-PT (Calame-GL is pending).

## Tool Features

### Metrics

- **Cosine Similarity**: Calculates the cosine similarity between the embeddings of the last layer associated with two text fragments, e.g., between a modelâ€™s generation and a reference text fragment.
- **MoverScore** \[[*code*](https://github.com/AIPHES/emnlp19-moverscore), [*paper*](https://arxiv.org/pdf/1909.02622)\]: Uses embeddings from a BERT model to calculate the *effort* required to transform one text into another. For example, it measures how difficult it is to turn a model's generation into a reference text. The lower the *effort*, the higher the similarity between texts.
- **BertScore** \[[*code*](https://github.com/Tiiiger/bert_score), [*paper*](https://arxiv.org/pdf/1904.09675)\]: Uses embeddings from a BERT model to compute a refined version of Cosine Similarity.
- **Surprisal** \[[code](https://github.com/kanishkamisra/minicons), [*paper*](https://arxiv.org/pdf/2203.13112)\]: Measures the *surprise* a model experiences when encountering a token or set of tokens. High values indicate that the model is unaccustomed to generating such tokens. This metric can be used to compare the knowledge of different models in the same language: models with lower surprisal values are theoretically better at the language than those with higher values (which are more *surprised* by the text).

### Tasks and Available Datasets

- **Multiple Choice QA**:
  - *OpenBookQA*: Four possible answers. A question is asked, and the correct answer must be selected.
  - *Belebele*: Four possible answers. A context is provided with diverse information, followed by a question with options where the correct answer can be deduced from the context.
- **Linguistic Acceptability**:  
  - *CoLA*: Contains sentences labeled as linguistically acceptable (1) or unacceptable (0). This allows for studying when a model is more likely to generate acceptable or unacceptable texts by comparing the probabilities it assigns to each type of sentence.
- **Generative Capabilities**:
  - *Calame*: A text fragment is provided, and the task is to complete the last word. The dataset is designed so that the last word should be unique, and the goal is to check whether the word generated by the model matches the reference word in the dataset for each fragment.

### Metrics by Dataset

|            |       Cosine Similarity       |     MoverScore     |      BertScore     |      Surprisal     |
|------------|:-----------------------------:|:------------------:|:------------------:|:------------------:|
| OpenBookQA | :heavy_check_mark:            | :heavy_check_mark: | :heavy_check_mark: |                    |
|  Belebele  | :heavy_check_mark:            | :heavy_check_mark: | :heavy_check_mark: |                    |
|   CoLA     |                               |                    |                    | :heavy_check_mark: |
|   Calame   |                               |                    |                    | :heavy_check_mark: |

### Datasets by Language

|            |                                   Galician                                  |                                         English                                        |                                         Catalan                                        |                                         Spanish                                        | Portuguese                                                                             |
|------------|:--------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|:-------------------------------------------------------------------------------------:|---------------------------------------------------------------------------------------|
| OpenBookQA | [openbookqa_gl](https://huggingface.co/datasets/proxectonos/openbookqa_gl) | [openbookqa](https://huggingface.co/datasets/cnut1648/openbookqa_retrieved_by_colbert) | [openbookqa_ca](https://huggingface.co/datasets/projecte-aina/openbookqa_ca)          | [openbookqa_es](https://huggingface.co/datasets/BSC-LT/openbookqa-es)                 | Pending                                                                               |
| Belebele   | [belebele_gl](https://huggingface.co/datasets/proxectonos/belebele_gl)     | [belebele_eng_Latn](https://huggingface.co/datasets/facebook/belebele/viewer/eng_Latn) | [belebele_cat_Latn](https://huggingface.co/datasets/facebook/belebele/viewer/cat_Latn) | [belebele_spa_Latn](https://huggingface.co/datasets/facebook/belebele/viewer/spa_Latn) | [belebele_por_Latn](https://huggingface.co/datasets/facebook/belebele/viewer/por_Latn) |
| CoLA       | [galcola](https://huggingface.co/datasets/proxectonos/galcola)             | [glue_cola](https://huggingface.co/datasets/nyu-mll/glue/viewer/cola)                 | [CatCoLA](https://huggingface.co/datasets/nbel/CatCoLA)                               | [EsCoLA](https://huggingface.co/datasets/nbel/EsCoLA)                                 |                                                                                       |
| Calame     |                                  Pending                                   |                                                                                       |                                                                                       |                                                                                       | [calame-pt](https://huggingface.co/datasets/NOVA-vision-language/calame-pt)           |
